---
title: "Research Highlights"
draft: false
language: en
---

## Privacy-preserving Language Model
In our evaluation algorithms for exploring potential vulnerabilities in large language models such as ChatGPT, Llama3, and Qwen, we can explore 10 times more. We combine various techniques including RedTeaming, Fuzzing, and Jailbreaking to exhaustively find potential privacy leakage. On the opposite side, we can defend against these existing vulnerabilities through differential privacy, unlearning, etc., better than existing solutions.

<div style="display: flex; justify-content: space-between;">
  <img src="https://github.com/HemligCA/HemligCA.github.io/research/invesetment.png" alt="Image 1" style="width: 32%;"/>
  <img src="https://github.com/HemligCA/HemligCA.github.io/research/length.png" alt="Image 2" style="width: 32%;"/>
  <img src="https://github.com/HemligCA/HemligCA.github.io/research/zlib.png" alt="Image 3" style="width: 32%;"/>
</div>
